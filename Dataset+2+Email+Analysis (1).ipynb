{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "email = pd.read_csv(\"/Users/deepikamulchandani/Downloads/DataSets2_10012017/email_persisting_employees.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>pc</th>\n",
       "      <th>to</th>\n",
       "      <th>cc</th>\n",
       "      <th>bcc</th>\n",
       "      <th>from</th>\n",
       "      <th>size</th>\n",
       "      <th>attachments</th>\n",
       "      <th>content</th>\n",
       "      <th>Does the employee still exist?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{R3I7-S4TX96FG-8219JWFF}</td>\n",
       "      <td>01/02/2010 07:11:45</td>\n",
       "      <td>LAP0338</td>\n",
       "      <td>PC-5758</td>\n",
       "      <td>Dean.Flynn.Hines@dtaa.com;Wade_Harrison@lockhe...</td>\n",
       "      <td>Nathaniel.Hunter.Heath@dtaa.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lynn.Adena.Pratt@dtaa.com</td>\n",
       "      <td>25830</td>\n",
       "      <td>0</td>\n",
       "      <td>middle f2 systems 4 july techniques powerful d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{R0R9-E4GL59IK-2907OSWJ}</td>\n",
       "      <td>01/02/2010 07:12:16</td>\n",
       "      <td>MOH0273</td>\n",
       "      <td>PC-6699</td>\n",
       "      <td>Odonnell-Gage@bellsouth.net</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MOH68@optonline.net</td>\n",
       "      <td>29942</td>\n",
       "      <td>0</td>\n",
       "      <td>the breaking called allied reservations former...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{G2B2-A8XY58CP-2847ZJZL}</td>\n",
       "      <td>01/02/2010 07:13:00</td>\n",
       "      <td>LAP0338</td>\n",
       "      <td>PC-5758</td>\n",
       "      <td>Penelope_Colon@netzero.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lynn_A_Pratt@earthlink.net</td>\n",
       "      <td>28780</td>\n",
       "      <td>0</td>\n",
       "      <td>slowly this uncinus winter beneath addition ex...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{A3A9-F4TH89AA-8318GFGK}</td>\n",
       "      <td>01/02/2010 07:13:17</td>\n",
       "      <td>LAP0338</td>\n",
       "      <td>PC-5758</td>\n",
       "      <td>Judith_Hayden@comcast.net</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lynn_A_Pratt@earthlink.net</td>\n",
       "      <td>21907</td>\n",
       "      <td>0</td>\n",
       "      <td>400 other difficult land cirrocumulus powered ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{E8B7-C8FZ88UF-2946RUQQ}</td>\n",
       "      <td>01/02/2010 07:13:28</td>\n",
       "      <td>MOH0273</td>\n",
       "      <td>PC-6699</td>\n",
       "      <td>Bond-Raymond@verizon.net;Alea_Ferrell@msn.com;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odonnell-Gage@bellsouth.net</td>\n",
       "      <td>MOH68@optonline.net</td>\n",
       "      <td>17319</td>\n",
       "      <td>0</td>\n",
       "      <td>this kmh october holliswood number advised unu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id                 date     user       pc  \\\n",
       "0  {R3I7-S4TX96FG-8219JWFF}  01/02/2010 07:11:45  LAP0338  PC-5758   \n",
       "1  {R0R9-E4GL59IK-2907OSWJ}  01/02/2010 07:12:16  MOH0273  PC-6699   \n",
       "2  {G2B2-A8XY58CP-2847ZJZL}  01/02/2010 07:13:00  LAP0338  PC-5758   \n",
       "3  {A3A9-F4TH89AA-8318GFGK}  01/02/2010 07:13:17  LAP0338  PC-5758   \n",
       "4  {E8B7-C8FZ88UF-2946RUQQ}  01/02/2010 07:13:28  MOH0273  PC-6699   \n",
       "\n",
       "                                                  to  \\\n",
       "0  Dean.Flynn.Hines@dtaa.com;Wade_Harrison@lockhe...   \n",
       "1                        Odonnell-Gage@bellsouth.net   \n",
       "2                         Penelope_Colon@netzero.com   \n",
       "3                          Judith_Hayden@comcast.net   \n",
       "4  Bond-Raymond@verizon.net;Alea_Ferrell@msn.com;...   \n",
       "\n",
       "                                cc                          bcc  \\\n",
       "0  Nathaniel.Hunter.Heath@dtaa.com                          NaN   \n",
       "1                              NaN                          NaN   \n",
       "2                              NaN                          NaN   \n",
       "3                              NaN                          NaN   \n",
       "4                              NaN  Odonnell-Gage@bellsouth.net   \n",
       "\n",
       "                         from   size  attachments  \\\n",
       "0   Lynn.Adena.Pratt@dtaa.com  25830            0   \n",
       "1         MOH68@optonline.net  29942            0   \n",
       "2  Lynn_A_Pratt@earthlink.net  28780            0   \n",
       "3  Lynn_A_Pratt@earthlink.net  21907            0   \n",
       "4         MOH68@optonline.net  17319            0   \n",
       "\n",
       "                                             content  \\\n",
       "0  middle f2 systems 4 july techniques powerful d...   \n",
       "1  the breaking called allied reservations former...   \n",
       "2  slowly this uncinus winter beneath addition ex...   \n",
       "3  400 other difficult land cirrocumulus powered ...   \n",
       "4  this kmh october holliswood number advised unu...   \n",
       "\n",
       "   Does the employee still exist?  \n",
       "0                               1  \n",
       "1                               1  \n",
       "2                               1  \n",
       "3                               1  \n",
       "4                               1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del email['Unnamed: 0']\n",
    "email.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = email['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          middle f2 systems 4 july techniques powerful d...\n",
       "1          the breaking called allied reservations former...\n",
       "2          slowly this uncinus winter beneath addition ex...\n",
       "3          400 other difficult land cirrocumulus powered ...\n",
       "4          this kmh october holliswood number advised unu...\n",
       "5          little equal k is group cannot though with lea...\n",
       "6          stroke menacing 115 five parents early continu...\n",
       "7          leading companys gained offers many mid oz scr...\n",
       "8          reception website due sold third recognition s...\n",
       "9          smaller weather responsible cemetery left coll...\n",
       "10         do potentially 2 5 through countries positivel...\n",
       "11         villepin five sharp well they history meant sh...\n",
       "12         he instill prehistoric occupies explains night...\n",
       "13         went could jackson awarding marine wii united ...\n",
       "14         connections program print campus midtown d ico...\n",
       "15         rested considered actions ronald according res...\n",
       "16         additional funeral 7 negative since quite has ...\n",
       "17         need there did comes named each giving star ho...\n",
       "18         since data english through store better search...\n",
       "19         orphan treatment somewhat steve worldwide augu...\n",
       "20         hong awarding label difficult still predicted ...\n",
       "21         proposed that overwhelming commission producin...\n",
       "22         three childhood michael described every sticki...\n",
       "23         island sick gabriel without guard religious pa...\n",
       "24         unrestricted 11 degree recipients 2009 saw fou...\n",
       "25         career of consisted inherently streetly positi...\n",
       "26         funeral phone in dedicated emotion lost concep...\n",
       "27         presentation then his thick six involving love...\n",
       "28         nominal canada landslide frauds together west ...\n",
       "29         oats stage sun achievements disturb would 210 ...\n",
       "                                 ...                        \n",
       "2388086    heavy what pay 15 potential tribune destroyers...\n",
       "2388087    deliberation copies hill digital schofield ord...\n",
       "2388088    several scene so hot edge meter town create la...\n",
       "2388089    primary new version as get sweet press she dea...\n",
       "2388090    emerald corporate absorbed nasa seals 16 hunte...\n",
       "2388091    supposed jackson praised well label self inter...\n",
       "2388092    else clear rocks architectural unusual reaches...\n",
       "2388093    mentioned support europe enthusiasts prior muc...\n",
       "2388094    magnitude volume scientists information less s...\n",
       "2388095    jumped formations difficult iv gather lavish d...\n",
       "2388096    prince prince prince prince prince prince prin...\n",
       "2388097    ernest government must often playwriting expec...\n",
       "2388098    too said store simon speculation number comes ...\n",
       "2388099    gradually intensification continuing diameter ...\n",
       "2388100    reorganized prone insurance reversed february ...\n",
       "2388101    knew felt knew time acknowledge rudimentary sa...\n",
       "2388102    typically final aggregates deteriorating 19 co...\n",
       "2388103    finally revenge outside continent preventing o...\n",
       "2388104    many after had see all roughly multiplicity sh...\n",
       "2388105    prince prince prince prince prince prince ahmo...\n",
       "2388106    tradition up class spa based gained contempora...\n",
       "2388107    in pursue these morn inspiration sheet minded ...\n",
       "2388108    monastery anything english metaphors general j...\n",
       "2388109    oz four they positively need available newly f...\n",
       "2388110    sudden maritime storm pinpointing across gradu...\n",
       "2388111    history designed stephen degree ignore them ad...\n",
       "2388112    prince prince ahmose ahmose ankh prince prince...\n",
       "2388113    lifted documents 65 declined revival 14 unprec...\n",
       "2388114    their official holmes face arranged among priz...\n",
       "2388115    most originally beautiful body word mind right...\n",
       "Name: content, Length: 2388116, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/deepikamulchandani/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(cont):\n",
    "    stop_free = \" \".join([i for i in cont.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "content_clean = [clean(cont).split() for cont in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(content_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_term_matrix = [dictionary.doc2bow(cont) for cont in content_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel = Lda(content_term_matrix, num_topics=30, id2word = dictionary, passes=1, chunksize = 2000, iterations = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.004*\"give\" + 0.004*\"create\" + 0.004*\"trust\" + 0.003*\"demanding\" + 0.003*\"water\"'), (1, u'0.004*\"towards\" + 0.004*\"1995\" + 0.003*\"far\" + 0.003*\"source\" + 0.003*\"information\"'), (2, u'0.003*\"called\" + 0.003*\"large\" + 0.003*\"six\" + 0.003*\"william\" + 0.003*\"began\"'), (3, u'0.005*\"effect\" + 0.005*\"since\" + 0.005*\"referred\" + 0.004*\"way\" + 0.004*\"village\"'), (4, u'0.763*\"prince\" + 0.085*\"ahmose\" + 0.082*\"ankh\" + 0.029*\"guidance\" + 0.002*\"marital\"'), (5, u'0.062*\"team\" + 0.056*\"degree\" + 0.056*\"job\" + 0.053*\"multiple\" + 0.053*\"year\"'), (6, u'0.004*\"brother\" + 0.004*\"original\" + 0.004*\"could\" + 0.004*\"around\" + 0.004*\"pre\"'), (7, u'0.038*\"customer\" + 0.005*\"child\" + 0.004*\"english\" + 0.004*\"history\" + 0.003*\"rank\"'), (8, u'0.004*\"october\" + 0.003*\"22\" + 0.003*\"25\" + 0.003*\"near\" + 0.003*\"following\"'), (9, u'0.059*\"experience\" + 0.057*\"required\" + 0.050*\"responsibility\" + 0.039*\"sale\" + 0.037*\"growth\"'), (10, u'0.049*\"skill\" + 0.007*\"process\" + 0.004*\"12\" + 0.004*\"despite\" + 0.004*\"19\"'), (11, u'0.032*\"interface\" + 0.011*\"engineer\" + 0.004*\"replaced\" + 0.004*\"permitted\" + 0.004*\"involving\"'), (12, u'0.009*\"refused\" + 0.009*\"constructed\" + 0.009*\"important\" + 0.009*\"defended\" + 0.009*\"stored\"'), (13, u'0.005*\"leave\" + 0.004*\"christian\" + 0.004*\"man\" + 0.004*\"word\" + 0.003*\"seen\"'), (14, u'0.004*\"rarely\" + 0.003*\"potential\" + 0.003*\"prominent\" + 0.003*\"howe\" + 0.003*\"younger\"'), (15, u'0.039*\"develop\" + 0.015*\"compensation\" + 0.013*\"process\" + 0.004*\"component\" + 0.004*\"eye\"'), (16, u'0.007*\"still\" + 0.006*\"right\" + 0.006*\"led\" + 0.005*\"half\" + 0.005*\"extended\"'), (17, u'0.011*\"online\" + 0.006*\"daughter\" + 0.005*\"sent\" + 0.005*\"know\" + 0.004*\"london\"'), (18, u'0.053*\"dynamic\" + 0.008*\"process\" + 0.007*\"reserve\" + 0.006*\"manner\" + 0.006*\"remarked\"'), (19, u'0.004*\"five\" + 0.004*\"known\" + 0.003*\"area\" + 0.003*\"three\" + 0.003*\"high\"'), (20, u'0.003*\"june\" + 0.003*\"20\" + 0.003*\"week\" + 0.003*\"date\" + 0.003*\"failure\"'), (21, u'0.007*\"relocation\" + 0.004*\"distinguished\" + 0.004*\"consisting\" + 0.004*\"lose\" + 0.004*\"must\"'), (22, u'0.005*\"result\" + 0.004*\"90\" + 0.004*\"path\" + 0.004*\"entered\" + 0.004*\"120\"'), (23, u'0.004*\"stephen\" + 0.004*\"richard\" + 0.003*\"digital\" + 0.003*\"whose\" + 0.003*\"century\"'), (24, u'0.039*\"contribute\" + 0.004*\"serving\" + 0.004*\"child\" + 0.004*\"dont\" + 0.003*\"following\"'), (25, u'0.003*\"prevent\" + 0.003*\"protected\" + 0.002*\"state\" + 0.002*\"encountered\" + 0.002*\"fall\"'), (26, u'0.013*\"analyze\" + 0.003*\"personal\" + 0.003*\"struck\" + 0.002*\"behind\" + 0.002*\"seven\"'), (27, u'0.054*\"resume\" + 0.016*\"starter\" + 0.005*\"critic\" + 0.005*\"february\" + 0.004*\"become\"'), (28, u'0.007*\"listed\" + 0.005*\"strike\" + 0.005*\"museum\" + 0.005*\"paper\" + 0.005*\"rocky\"'), (29, u'0.071*\"industry\" + 0.067*\"management\" + 0.007*\"diameter\" + 0.005*\"whereas\" + 0.005*\"white\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=30, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel.save('/Users/deepikamulchandani/Downloads/DataSets2_10012017/lda_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.002*\"order\" + 0.002*\"method\" + 0.002*\"suitable\" + 0.002*\"bad\" + 0.002*\"condition\"'), (1, u'0.035*\"required\" + 0.035*\"experience\" + 0.033*\"year\" + 0.032*\"skill\" + 0.032*\"degree\"'), (2, u'0.022*\"growth\" + 0.020*\"initiative\" + 0.010*\"salary\" + 0.003*\"relocation\" + 0.003*\"work\"'), (3, u'0.003*\"36\" + 0.003*\"building\" + 0.003*\"st\" + 0.003*\"foot\" + 0.003*\"concern\"'), (4, u'0.044*\"multiple\" + 0.041*\"team\" + 0.037*\"sale\" + 0.032*\"technology\" + 0.015*\"compensation\"'), (5, u'0.003*\"found\" + 0.003*\"life\" + 0.003*\"new\" + 0.003*\"head\" + 0.003*\"five\"'), (6, u'0.019*\"guidance\" + 0.005*\"offer\" + 0.004*\"london\" + 0.003*\"game\" + 0.003*\"world\"'), (7, u'0.002*\"ad\" + 0.002*\"shakur\" + 0.002*\"trust\" + 0.002*\"previously\" + 0.002*\"appeared\"'), (8, u'0.003*\"even\" + 0.003*\"led\" + 0.003*\"hand\" + 0.003*\"identified\" + 0.003*\"order\"'), (9, u'0.003*\"estimated\" + 0.003*\"2\" + 0.003*\"part\" + 0.003*\"still\" + 0.003*\"13\"'), (10, u'0.003*\"royal\" + 0.002*\"22\" + 0.002*\"following\" + 0.002*\"younger\" + 0.002*\"near\"'), (11, u'0.056*\"resume\" + 0.043*\"concept\" + 0.035*\"benefit\" + 0.012*\"team\" + 0.008*\"call\"'), (12, u'0.046*\"management\" + 0.011*\"engineer\" + 0.007*\"included\" + 0.006*\"attention\" + 0.006*\"refused\"'), (13, u'0.009*\"analyze\" + 0.003*\"keeping\" + 0.003*\"fund\" + 0.003*\"week\" + 0.003*\"celebration\"'), (14, u'0.003*\"predicted\" + 0.003*\"voted\" + 0.003*\"entertainment\" + 0.002*\"stated\" + 0.002*\"equally\"'), (15, u'0.026*\"contribute\" + 0.022*\"passion\" + 0.005*\"engineer\" + 0.003*\"sent\" + 0.003*\"official\"'), (16, u'0.783*\"prince\" + 0.086*\"ahmose\" + 0.086*\"ankh\" + 0.002*\"reduce\" + 0.002*\"marital\"'), (17, u'0.010*\"interface\" + 0.005*\"starter\" + 0.002*\"known\" + 0.002*\"small\" + 0.002*\"low\"'), (18, u'0.006*\"fight\" + 0.005*\"resolution\" + 0.004*\"together\" + 0.004*\"awarded\" + 0.004*\"determined\"'), (19, u'0.004*\"online\" + 0.002*\"half\" + 0.002*\"new\" + 0.002*\"month\" + 0.002*\"five\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=20, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "email_re = pd.read_csv(\"/Users/deepikamulchandani/Downloads/DataSets2_10012017/email_removed_employees.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241863"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_re.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = email_re['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(cont):\n",
    "    stop_free = \" \".join([i for i in cont.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "content_clean_re = [clean(cont).split() for cont in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary_re = corpora.Dictionary(content_clean_re)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_term_matrix_re = [dictionary_re.doc2bow(cont) for cont in content_clean_re]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel_re = Lda(content_term_matrix_re, num_topics=30, id2word = dictionary_re, passes=1, chunksize = 2000, iterations = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.010*\"heat\" + 0.009*\"1975\" + 0.009*\"1995\" + 0.008*\"blow\" + 0.008*\"maintain\"'), (1, u'0.021*\"guidance\" + 0.005*\"series\" + 0.004*\"laid\" + 0.004*\"opposed\" + 0.004*\"offer\"'), (2, u'0.079*\"skill\" + 0.006*\"brian\" + 0.005*\"peaked\" + 0.005*\"bullet\" + 0.005*\"wait\"'), (3, u'0.003*\"voice\" + 0.003*\"production\" + 0.003*\"major\" + 0.003*\"possibly\" + 0.003*\"car\"'), (4, u'0.014*\"management\" + 0.013*\"technology\" + 0.005*\"visual\" + 0.003*\"non\" + 0.003*\"make\"'), (5, u'0.004*\"le\" + 0.004*\"small\" + 0.004*\"total\" + 0.003*\"across\" + 0.003*\"least\"'), (6, u'0.003*\"led\" + 0.003*\"five\" + 0.003*\"next\" + 0.003*\"release\" + 0.003*\"senior\"'), (7, u'0.089*\"required\" + 0.080*\"experience\" + 0.079*\"team\" + 0.072*\"resume\" + 0.030*\"process\"'), (8, u'0.044*\"degree\" + 0.042*\"year\" + 0.037*\"multiple\" + 0.037*\"job\" + 0.036*\"multitask\"'), (9, u'0.018*\"starter\" + 0.006*\"paul\" + 0.006*\"bring\" + 0.005*\"vehicle\" + 0.005*\"section\"'), (10, u'0.003*\"become\" + 0.003*\"throughout\" + 0.003*\"history\" + 0.003*\"area\" + 0.003*\"month\"'), (11, u'0.005*\"next\" + 0.005*\"central\" + 0.005*\"close\" + 0.005*\"around\" + 0.005*\"chance\"'), (12, u'0.004*\"state\" + 0.004*\"left\" + 0.004*\"major\" + 0.004*\"via\" + 0.004*\"word\"'), (13, u'0.003*\"died\" + 0.003*\"history\" + 0.003*\"accepted\" + 0.003*\"authority\" + 0.003*\"decade\"'), (14, u'0.004*\"woman\" + 0.003*\"especially\" + 0.003*\"term\" + 0.003*\"similar\" + 0.003*\"sometimes\"'), (15, u'0.026*\"contribute\" + 0.003*\"clean\" + 0.003*\"produced\" + 0.003*\"considered\" + 0.003*\"case\"'), (16, u'0.005*\"inner\" + 0.005*\"make\" + 0.004*\"book\" + 0.004*\"gate\" + 0.004*\"could\"'), (17, u'0.003*\"weekly\" + 0.003*\"batman\" + 0.003*\"dont\" + 0.002*\"opinion\" + 0.002*\"theater\"'), (18, u'0.003*\"prisoner\" + 0.003*\"flying\" + 0.003*\"fall\" + 0.003*\"several\" + 0.002*\"access\"'), (19, u'0.004*\"scholar\" + 0.003*\"wanted\" + 0.003*\"reduced\" + 0.003*\"project\" + 0.003*\"right\"'), (20, u'0.006*\"exposed\" + 0.005*\"supply\" + 0.005*\"destruction\" + 0.005*\"modern\" + 0.005*\"surface\"'), (21, u'0.020*\"expert\" + 0.017*\"analyze\" + 0.014*\"salary\" + 0.012*\"engineer\" + 0.006*\"special\"'), (22, u'0.003*\"increase\" + 0.003*\"lighting\" + 0.003*\"status\" + 0.003*\"remove\" + 0.003*\"discrepancy\"'), (23, u'0.749*\"prince\" + 0.082*\"ankh\" + 0.081*\"ahmose\" + 0.036*\"passion\" + 0.004*\"130\"'), (24, u'0.004*\"opportunity\" + 0.004*\"arranged\" + 0.004*\"intended\" + 0.004*\"transport\" + 0.004*\"train\"'), (25, u'0.067*\"benefit\" + 0.014*\"headed\" + 0.013*\"positive\" + 0.012*\"engine\" + 0.011*\"room\"'), (26, u'0.004*\"september\" + 0.004*\"way\" + 0.004*\"present\" + 0.004*\"behind\" + 0.004*\"journal\"'), (27, u'0.063*\"responsibility\" + 0.054*\"develop\" + 0.049*\"equivalent\" + 0.026*\"self\" + 0.024*\"compensation\"'), (28, u'0.006*\"attempt\" + 0.006*\"made\" + 0.005*\"second\" + 0.005*\"reached\" + 0.005*\"included\"'), (29, u'0.005*\"plan\" + 0.004*\"london\" + 0.004*\"produced\" + 0.004*\"music\" + 0.004*\"standard\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel_re.print_topics(num_topics=30, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "email_Sample = email.sample(n = 241863, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241863"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_Sample.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_Sample = email['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(cont):\n",
    "    stop_free = \" \".join([i for i in cont.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "content_Sample_clean = [clean(cont).split() for cont in content_Sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary_Sample = corpora.Dictionary(content_Sample_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_term_matrix_Sample = [dictionary_Sample.doc2bow(cont) for cont in content_Sample_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel_Sample = Lda(content_term_matrix_Sample, num_topics=30, id2word = dictionary_Sample, passes=1, chunksize = 2000, iterations = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "email_domain = pd.read_csv(\"/Users/deepikamulchandani/Downloads/DataSets2_10012017/email_onlypersonaldomains.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=1000,\n",
    "                                   stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = tfidf_vectorizer.fit_transform(email_domain['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=30, max_iter=100,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=100, mean_change_tol=0.001,\n",
       "             n_components=30, n_jobs=1, n_topics=None, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_feature_names = tfidf_vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: skills multitask degree job develop\n",
      "Topic #1: member list helped fully coast\n",
      "Topic #2: forced character education search hot\n",
      "Topic #3: starter changes intended acquired immediate\n",
      "Topic #4: season work claimed cut 2011\n",
      "Topic #5: force fall formed reported impact\n",
      "Topic #6: site passed decided deep increased\n",
      "Topic #7: grow distinct results roughly raising\n",
      "Topic #8: designated operations vavau sending originated\n",
      "Topic #9: experience responsibilities team sales concepts\n",
      "Topic #10: law figures completely letter know\n",
      "Topic #11: allowing primary current variety brief\n",
      "Topic #12: dynamic holding traditional ranked contains\n",
      "Topic #13: preferred army combined private share\n",
      "Topic #14: taking san dismissed miles account\n",
      "Topic #15: platform benefits permanent relocation covered\n",
      "Topic #16: salary engineer 60 story chief\n",
      "Topic #17: expert visual surviving event social\n",
      "Topic #18: resume career friends incident needed\n",
      "Topic #19: built news completed giving managed\n",
      "Topic #20: remains attacks gained capable bringing\n",
      "Topic #21: right provided true growing meant\n",
      "Topic #22: think london 1999 location lose\n",
      "Topic #23: leave previously technique notably model\n",
      "Topic #24: middle charles turned unknown february\n",
      "Topic #25: majority sides britain potential 20th\n",
      "Topic #26: required contribute technologies interface start\n",
      "Topic #27: analyze capital related 1990 blue\n",
      "Topic #28: school losing begin conference republic\n",
      "Topic #29: advantage competition possession female contributed\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda,tf_feature_names,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
